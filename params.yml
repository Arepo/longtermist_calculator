# The full_calc.py script implements the perils-focused model described in this essay:
# https://forum.effectivealtruism.org/posts/YnBwoNNqe6knBJH8p/modelling-civilisation-after-a-catastrophe
# This file contains the numeric parameters and explains the default choices. The reader is actively
# encouraged to substitute their own parameters to test the effect. Any one of these parameters
# could be, arguably should be - and often have been - the subject of a substantial research project.
# In general I've set them to my best guess, which has come from some combination of the research
# I've found on the subject and eyeballing graphs on Desmos(a browser-based graphing
# site) to come up with an intuitively convincing value per year.

# Substantially the most complex states are the 'time of perils' (any period in which we've developed
# potentially civilisation-harming weaponry and before we've settled other planets) and 'multiplanetary'
# (any period in which we've settled at least one other planet, or developed a technologically self-
# sufficient and geographically isolated equivalent that could both survive a major war that
# destroyed the rest of civilisation and reliably and quickly recolonise Earth afterwards)

# To capture my intuitions about various risks as simply as possible without losing anything that
# feels important, I'm using four types of graphs to describe exit probabilities from those eras.
# I'll describe each of those graphs below alongside the first set of relevant parameters, and for
# all sets of parameters that relate to one, I'll provide a link to a Desmos visualisation that lets
# you easily see the results of tweaking the parameters. Note: all Desmos graphs assume k=1 - that is,
# they give visualisations of my estimates for our current situation on which the estimates for
# future civilisations could be based.

# An exponential decay curve of the form A(1 - B)^(x - C) + D
#    Where A is a stretch in the Y direction (which by default determines the the y-value at x=0)
#    B is a value between 0 and 1 giving the steepness of the decay (0 is basically no decay)
#    C is a translation in the x-direction
#    D is a translation in the y-direction (determining the minimum value the risk can reduce and
#      adding to the starting value)

# A pseudo-gamma distribution, albeit with stretches and translations as above (implemented via
# scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html). FLAG: this
# can't be a strict probability distribution given the possibility of revisiting progress years on
# the x_axis

preperils:
  # Parameters for the three technological states of civilisation with pre-perils technology
  # (for example, nukes, biotech), to wit 'survival', 'preindustrial' and 'industrial'.

  # I expect the probability of extinction in these states to to decrease slightly with the value of
  # k, given  civilisations in the state have evidently survived to reach the perils state. That said,
  # I don't think it's correct to include that assumption in our priors (ie this file), so most of
  # the functions informed by these parameters are very simple.
  survival:
    base_estimate: 0.0003 # I've just lifted this from one of Rodriguez's most pessimistic scenarios
    # in this post: https://forum.effectivealtruism.org/posts/GsjmufaebreiaivF7/what-is-the-likelihood-that-civilizational-collapse-would
    # I don't think the first two scenarios really describe a state of 'survival' for much the reasons
    # she describes. It could be much higher, given model uncertainty or its sensitivity to
    # lower numbers of surviving humans or much lower if we define the survival state more broadly.
    # TODO - it would save a decent amount of runtime and probably not change much to set this to 0
    # (or translate it as a multiplier on preindustrial states)
  preindustrial:
    # There are various different suggested values in the comments below.

    # Uncomment a value below to taste of expected time in years. The estimates of expected time to
    # recover come from Luisa Rodriguez's post,
    # https://forum.effectivealtruism.org/posts/Nc9fCzjBKYDaDJGiX/what-is-the-likelihood-that-civilizational-collapse-would-1
    # She now claims that she has updated towards 'technological stagnation', for which 'The biggest
    # reason is probably the risk of extreme, long-lasting climate change. It seems possible that
    # anthropogenic climate change could cause global warming extreme enough that agriculture would
    # become much more difficult than it was for early agriculturalists. Temperatures wouldn’t return to
    # current levels for hundreds of thousands of years, so if the warmer temperatures were much less
    # conducive to recovering agriculture and downstream technological developments, humanity might be
    # stagnant for millennia.' Unfortunately she doesn't further quantify this shift in estimates, but
    # hers is still the most comprehensive piece I know on the subject. So the first values are based
    # on her initial estimates (not they're in the order they appear in the post, not ascending):

    # The value used in the code is 1/<some value>, but we can't do arithmetic in
    # Yaml, so the param is just the denominator:

    # 'If we think recovery time is limited by...'
    # 'Agricultural rev. and industrialization take as long as they did the first time'
    # expected_time_in_years: 300_000

    # 'Agricultural civilization returns quickly, industrial revolution takes as long as it did the
    # first time'
    # expected_time_in_years: 500 # Lower end of her range in this scenario
    # expected_time_in_years: 30_000 # Upper end of her range in this scenario

    # 'Inside view — If we assume that existing physical and human capital would accelerate the speed
    # of re-industrialization relative to the base rate'

    # 'Best case guess - Assumes the British industrial revolution happened about when we’d have expected'
    # expected_time_in_years: 100 # Lower end of her range in this scenario
    # expected_time_in_years: 3700 # Upper end of her range in this scenario

    # 'Pessimistic case guess - Assumes we got very lucky with the British industrial revolution'
    # expected_time_in_years: 1000 # Lower end of her range in this scenario
    # expected_time_in_years: 33_000 # Upper end of her range in this scenario

    # To account for her remarks about technological stagnation, I just naively add 10000 years to
    # each of the above.
    # expected_time_in_years: 10_100
    # expected_time_in_years: 10_500
    # expected_time_in_years: 11_000
    expected_time_in_years: 13_700
    # expected_time_in_years: 40_000
    # expected_time_in_years: 43_000
    # expected_time_in_years: 310_000

    # The below estimates of annual extinction rate come from this paper:
    # https://www.nature.com/articles/s41598-019-47540-7 - quotes from the same. Uncomment to taste.
    # Note that for narrative reasons they're in the order they appeared in that article, not ascending
    # extinction_probability_per_year_denominator = 14_000
    # 'Assuming a 200 thousand year (kyr) survival time, we can be exceptionally confident that rates
    # do not exceed 6.9 * 10^−5. This corresponds to an annual extinction probability below roughly 1 in 14,000.'

    # extinction_probability_per_year_denominator = 22_800
    # 'Extinction can be represented by the exponential distribution with constant extinction rate μ...
    # Using the fossil dated to 315ka as a starting point for humanity gives an upper bound
    # of μ < 4.4 * 10^−5, corresponding to an annual extinction probability below 1 in 22,800'

    # extinction_probability_per_year_denominator = 140_000
    # 'Using the emergence of Homo as our starting point pushes the initial bound back a full order of
    # magnitude, resulting in an annual extinction probability below 1 in 140,000.'

    extinction_probability_per_year_denominator: 87_000
    # 'We can also relax the one in million relative likelihood constraint and derive less conservative
    # upper bounds. An alternative bound would be rates with relative likelihood below 10−1 (1 in 10)
    # when compared to the baseline rate of 10−8. If we assume humanity has lasted 200kyr, we
    # obtain a bound of μ < 1.2 * 10^−5, corresponding to an annual extinction probability below 1 in 87,000.'

    # extinction_probability_per_year_denominator = 870_000
    # 'Using the 2Myr origin of Homo strengthens the bound by an order of magnitude in a
    # similar way and produces annual extinction probabilities below 1 in 870,000.'

  industrial:
    # For extinction probability per year, we can start with the base rates given in the previous
    # calculation, and then multiply them by some factor based on whether we think industry would make
    # humans more or less resilient.

    # The value used in the code is 1/<some denominator>, but we can't do arithmetic in
    # Yaml, so the param is the denominator:
    # base_annual_extinction_probability_denominator: 14_000
    # base_annual_extinction_probability_denominator: 22_800
    base_annual_extinction_probability_denominator: 87_000
    # base_annual_extinction_probability_denominator: 140_000
    # base_annual_extinction_probability_denominator: 870_000

    annual_extinction_probability_multiplier: 0.7 # Intuition based on the reasoning below:
    # This paper estimates that British grain output approxmately doubled between ~1760-1850, and had
    # approximately doubled over the 300 years before that:
    # https://www.researchgate.net/publication/228043115_Yields_Per_Acre_in_English_Agriculture_1250-1860_Evidence_from_Labour_Inputs
    # So if we assume losses of food are uncorrelated with each other, and the majority of the world
    # would go through a similar transition at once, that would suggest a lower bound on this multiplier
    # of ~0.25. Obviously the rest of the world was slower, though - going by these estimates of historical
    # *wheat* yield, the US took til about the 1930s to start catching up:
    # https://www.researchgate.net/figure/Wheat-Yields-1800-2004_fig1_263620307
    # https://www.agry.purdue.edu/ext/corn/news/timeless/yieldtrends.html
    # Declining resources might slow down the spread of agricultural improvements, though. Also,
    # surplus food production might not matter much for extreme events such as a supervolcano that blocked out
    # the sky for many years - although it might incentivise surplus food preservation. I suspect
    # such events constitute the majority of pre-modern extinction risk.

    # For expected time in years, we have to make some strong assumptions, so I suggest two sets of
    # values, representing a pessimistic and an optimistic scenario:
    pessimistic:
      # Pessimistic scenario

      # In this scenario, used by default in the code, I assume all knowledge of previous civilisations' technology is either lost or
      # made useless by different resource constraints. Thus I imagine the original ~145 years for this
      # transition is stretched substantially by the decline in resource availability - most strongly so
      # from the initial lack of fossil fuels in reboot 1 and from phosphorus in subsequent reboots, then
      # somewhat more gently as rare earths etc are gradually left in unusable states.
      first_reboot_expected_time_in_years: 1450 # I mostly arbitrarily assume ten times the duration for rebooting with no oil, much less
      # coal that's more expensive to mine, and maybe 10% of the energy of our current civilisation
      # embodied in landfill:
      # https://scitechdaily.com/scientists-estimate-that-the-embodied-energy-of-waste-plastics-equates-to-12-of-u-s-industrial-energy-use/
      # Dartnell envisions what the process might look like here:
      # https://aeon.co/essays/could-we-reboot-a-modern-civilisation-without-fossil-fuels
      # He gives no probability estimates, but uses phrases like 'For a society to stand any chance of
      # industrialising under such conditions' and 'an industrial revolution without coal would be, at
      # a minimum, very difficult', suggesting he might think it's unlikely to *ever* happen.
      second_reboot_expected_time_in_years: 2900 # I imagine this disproportionately stretched again
      # by the complete absence of coal and other easily depletable resources. In particular easily
      # phosphorus accessible rock phosphorus would be gone (see discussion between John Halstead and
      # David Denkenberger here):
      # https://forum.effectivealtruism.org/posts/rtoGkzQkAhymErh2Q/are-we-going-to-run-out-of-phosphorous
      stretch_per_reboot: 1.5 # How much longer/less long would it take a typical reboot than in the
      # previous one to develop modern technology given resource-depletion considerations
    optimistic:
      # Optimistic scenario

      # In this scenario (needs to be manually enabled in the code - uncomment the relevant variables
      # in the extinction_given_industrial function in preperils.py if you want to run it) I assume
      # that the absence of fossil fuels/other resources is much less punitive and that enough
      # knowledge from previous civilisations is retained to actually speed up this transition
      # in the first couple of reboots
      stretch_per_reboot: 1.2
      base_expected_time_in_years: 100

perils:
  # Parameters for nodes within 'times of perils'

  # p is a 'progress year' - a year of real time at a certain level of technology, such that the time
  # of perils starts at p = 0, and we generally advance by one unit per year, but can regress by 0 up
  # to p years within the time of perils given 'minor' disasters.

  # Params governing transitional probabilities from perils to each of the nested states. Ensure that
  # the sum of the probabilities of all possible exits for any value of p is <= 1, where x=p and
  # probability of exit = y


  stretch_per_reboot: 1.5 # How much longer/less long would it take a typical reboot than in the
    # previous one to develop modern technology given considerations of headwinds from eg resource
    # depletion, environmental damage, and tailwinds from knowledge left over from previous
    # civilisations

  extinction:
    # This is the addition of an S-curve, representing persistent technological threats such as
    # biopandemics, to a pseudo-gamma distribution representing the probability per year of
    # developing AI, multiplied by the probability that if it's developed at in that year it
    # (ultimately) kills us all. The graphs are described in the relevant subsections below.

    # TODO/FLAG: to properly represent the intuition that AI has one chance to be transformative
    # and then fades as both an x-risk or as a source of existential security, I think we'd need to
    # create an extra pair of state-classes: post-AI perils and post-AI multiplanetary. Using the
    # current approach, that would add a fair bit of complexity to the code and give it a runtime of
    # O(MN^3), and given that it's already chugging at O(MN^2), for now I'm using the
    # separate-but-related pseudo-gamma-distribution graphs for AI-caused extinction and
    # AI-caused existential security described below.

    # The graph of the S-curve, the gammaoid curve, and the addition of the two with the values below
    # is at https://www.desmos.com/calculator/ocq9eq71g4

    # It might be interesting to remove the AI arc, which bakes in a tonne of highly speculative assumptions,
    # to compare what happens when we live in a world in which lacks either the motivation or capacity
    # to wipe us out - since unlike the other risks extinction, this is arguably somewhat predetermined,
    # such that probability estimates come from uncertainty rather than ongoing risks per year.
    # Similarly it might be interesting to compare against setting the probability of this transition
    # to 0, to represent a world in which AI will either be much harder to develop than we imagine,
    # or isn't radically transformative (eg it speeds up the development of human civilisation for better
    # or worse without changing the trajectory).

    # As with preperils probabilities of transition to extinction, this might evidentially diminish
    # with k, but I'm keeping that out of the priors.
    non_ai_causes:
      # This is the S-curve representing persistent technological threats.
      # The curve has the form A / (1 + 1 / B * (x - C)^-D * e^ -(1 / B * (x - C)))
      #   Where A is a stretch in the y-direction, represented by y_stretch below
      #   B is a stretch in the x-direction, represented by base_x_stretch below
      #   C is a translation parallel to the x-axis, represented by x_translation below
      #   D is a somewhat nebulous 'sharpness' parameter, which determines the steepness of the
      #     centre of the S-curve. This param might be overfitting, but makes it possible to align
      #     the curve with the actual historical growth of relevant technologies (eg nuclear arsenals)
      y_stretch: 0.04 # The initial max annual risk from non-AI causes.
        # Based on the highest estimate for the present day on the existential risks database:
        # https://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit#gid=0
      base_x_stretch: 118 # Set the initial shape of the graph to something plausibly consistent
      # with our current perceived risk - this will be multiplied by stretch_per_reboot ** (k + 1)
      x_translation: 15
      sharpness: 2.5
    agi_development:
      # Using a modified gamma distribution function as the simplest example of a long-tail
      # distribution I could find, to represent the probability distribution of when AGI is developed
      # conditional on us surviving long enough in a technological state.

      # The effect of the values on the curve's shape aren't particularly intuitive (to me, at
      # least), but you can play with them at https://www.desmos.com/calculator/6774lbip58
      shape: 2.4 # Influenced by on Ajeya Cotra's median estimate of 2040 (https://www.lesswrong.com/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines),
      # but with substantially more uncertainty about the upper limit of how long it could take.
      scale: 20 # TODO probably multiply this by stretch_per_reboot
      x_translation: 70 # Approximately the current year - used to translate both this and the graph
      # below. This implies the probability of having developed AI by this level of technology was
      # basically 0.
    agi_threat:
      # The probability that if AI is developed in progress year x, it goes on to wipe us out
      # Graph at https://www.desmos.com/calculator/xzmt0dkl0e
      # I've given this conditional probability as an exponential decay of the form
      # A * (1 - B)^-(x - C)
      #   Where A is the starting value after translation, represented by max_threat below
      #   B governs the rate at which the conditional probability decays, represented by
      #     threat_decay_rate below
      #   And C is the translation parallel to the x_axis, setting what value of X we think of as 'today',
      #     represented by the x_translation of agi_development above
      max_threat: 0.8 # The probability that it would wipe us out if it were developed today
      threat_decay_rate: 0.02 # This value relates to the value of AI safety work - if the per-year risk of
      # extinction conditional on AGI being developed decrease a lot, it implies we expect such work
      # will yield good results (although it could also just imply eg that a world in which AI is harder
      # to develop is a world in which fast takeoff is less likely)

  survival:
    # You can interact with the Desmos graph with these specific values at
    # https://www.desmos.com/calculator/21twtfavxk
    base_x_stretch: 91 # Sets the shape of the graph before multiplying by stretch_per_reboot. This
    # value puts us slightly above 1/2way to max risk in the present day, technology-wise
    y_stretch: 0.000_005 # Sets the max probability per year of this transition. This seems like
    # substantially the lowest probability perils exit, requiring a pinhead balance of maybe 99.99%
    # killed, but not immediate extinction
    x_translation: 0 # How many progress years into the time of perils does the risk of regressing
    # to a survival state rise above 0?
    sharpness: 2.4

  preindustrial:
    # You can interact with the Desmos graph for these specific values at
    # https://www.desmos.com/calculator/xgouzeyehb
    base_x_stretch: 20 # This value puts us slightly above 1/2way to max risk in the present day,
    # technology-wise
    y_stretch: 0.00045 # Sets the max probability per year of this transition. Ord's estimate of a
    # 0.05 chance of full scale nuclear war in the next century, given at
    # https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/#transcript
    # implies a ~0.0005 average chance per year. I assume it reaches double that at peak, and that
    # such a war would move us to preindustrial with 0.3 probability. Then I add half as much again for
    # the combined chance of transitioning to preindustrial via any other catastrophe
    x_translation: 0 # How many progress years into the time of perils does the risk of regressing
    # to a preindustrial state rise above 0?
    sharpness: 2 # A parameter determining the steepness of the centre of the S-curve. Set just by
    # eyeballing Desmos no substantive reasoning, and intuiting that it should climb earlier than
    # the probability of transitioning to to survival

  industrial:
    # You can interact with the Desmos graph for these specific values at
    # https://www.desmos.com/calculator/nhnbxrczk0
    base_x_stretch: 69 # Puts us approx 3/4 of the way to max annual risk, technology-wise
    y_stretch: 0.0015 # I assume a 0.1 total chance of a catastrophe that would wipe out billions in the
    # next century, giving a ~0.001 average chance per year. I assume it reaches triple that at peak.
    # and that such an event would move us to preindustrial with 0.5 probability.
    x_translation: 8 #  How many progress years into the time of perils does the risk of regressing
    # to an industrial state rise above 0? This sets it around the time the US would have reached
    # 1000 nuclear warheads.
    sharpness: 2

  progress_year_n:
    # Parameters determining the probability of transitioning the probability of transitioning within
    # the time of perils from progress-year p to progress-year n.

    # The simplest intuitive way I can think of to deal with this is to assume the probability of
    # regressing decreases exponentially with the number of years we regressing, eg for p = 3, given that
    # there has been some intra-perils regression we might say the probability of that regression is 1/15,
    # 2/15, 4/15, and 8/15 respectively for 'regressions' to p = 0, 1, 2, and 3.

    # More generally, we woud say that, given some intra-perils regression, the probability of a
    # regression to exactly progress-year n is a weighting_for_progress_year_n = <some weighting_decay_rate>**n,
    # divided by the sum of weighting_for_progress_year_n for all valid values of n.

    # I'm not sure this is a very convincing algorithm. Based on global GDP, we've arguably regressed
    # in about 4 calendar years since 1961, when the world bank started tracking global data and perhaps
    # 5 times in the 20th century based on UK data between about 0 and 2 progress years each time, and about . For comparatively tiny
    # values of weighting_decay_rate (ie barely above 1), being limited to such small regressions looks
    # incredibly unlikely. For higher values of weighting_decay_rate, it puts the total probability of
    # regressing more than a few years at a far lower value than the probability of a milestone
    # regression - a regression to an earlier technological state - which seems wrong.

    # A simple alternative that errs way too much in the other direction would be a linear decrease given by
    # an arithmetic progression. This seems like a much worse fit for the data, but I include that
    # version, commented out below, as a way of getting an upper bound on the significance of regressions
    # within the time of perils.

    # You can play with these numbers on Desmos. The green graph represents the probability of
    # regressing from progress year x to progress year n, such that 0 <= n <= x, and the red
    # graph shows the same thing with a different x-axis - the probability of regressing from progress year m to progress year
    # x, such that 0 <= x <= m: https://www.desmos.com/calculator/olr4mepouy

    any_regression: 0.026 # Could be a function of k and/or p - but seems ok to treat as a constant
                 # (at any given time using all the nukes would, more or less by definition, at
                 # least revert us to year 0 of the time of perils). This value is based on it
                 # roughly happening twice in 77 years on these graphs
                 # https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG
                 # https://ourworldindata.org/grapher/world-gdp-over-the-last-two-millennia?time=1940..2015
                 # See also UK GDP, for which reliable data goes back farther but is unsurprisingly
                 # more uneven: https://ourworldindata.org/grapher/total-gdp-in-the-uk-since-1270
    geometric_base: 1.4 # Higher gives higher probability that given such a loss we'll lose a
    # smaller number of progress years: 2 would mean regressing to year n is 2x
    # as likely as regressing to year n-1. I chose the current value fairly arbitrarily, by
    # looking at this graph - https://data.worldbank.org/indicator/NY.GDP.MKTP.KD.ZG, treating
    # 1975, and 1982 as a regression of 0 years, 2009 as a regression of 1, and 2020 as a
    # regression of 2, and looking for a probability outcome slightly below that (to account for
    # eg survivor bias, and selection effects from starting to count immediately *after* WWII).

  multiplanetary:
    # Meanwhile I also tried to get them looking somehwhat above Metaculus's 60% probability
    # for a colony by 2100 (since that's absolute probability, whereas this is an estimate of
    # given that we successfully continue advancing through the time of perils). For that I just
    # looked at probability of success over 50 years at the probability given at the midpoint from
    # 2050 to 2100, which is about 80%
    # (https://www.metaculus.com/questions/1432/will-humans-have-a-sustainable-off-world-presence-by-2100/)
    # on the probabilities for 2090 respectively.
    base_x_stretch: 75
    x_translation: 80 # Robert Zubrin seems to have been the first person to make a practical proposal to settle
    # Mars, and that was around 1990, and somewhat similar to SpaceX's plan. In his most optimistic
    # vision, the program would have taken maybe
    # a few years to get going, but have been slower to accelerate than Musk's idea, which Musk thought
    # in about 2020 was doable by about 2050. So if Zubrin's plan had been followed enthusiastically,
    # it might in the absolute best case have got there about 25 years earlier.
    y_stretch: 0.07 # Meaning a high tech civilisation could create a new settlement about once every 15
                    # years, given somewhere nearby to expand to
    sharpness: 2 # My default value when I'm not matching to historical data

  interstellar:
    # I treat this as being possible as a direct transition only if we develop AGI which becomes
    # a benign overseer to humanity, which will ensure our longterm flourishing. I don't have a strong
    # intuition about how or whether this would change over time (more time to AI development might
    # mean AI is less impressive in its abilities, or that there's a larger range of possible first
    # movers), so I'm just treating it as a constant probability of getting enough value
    # lock-in that our interstellar future becomes assured.
    value_lock_in_given_ai: 0.02 # My sense is that AI predictions mostly fall into
    # Camp A: 'it's going to be an overwhelmingly powerful new force of nature that will very likely
    # kill us all' and Camp B: 'it's going to be a very powerful tool which doesn't fundamentally
    # change human dynamics'. On either view, this outcome seems very unlikely, though not
    # inconceivable.

multiplanetary:
  # Params governing transitional probabilities from multiplanetary state to each of the nested
  # states. Note the definition from the post above of a 'planet' as 'a self-sustaining settlement
  # capable of continuing in an advanced enough technological state to settle other "planets" even
  # if the others disappeared, which is physically isolated enough to be unaffected by at least one
  # type of technological milestone catastrophe impacting the other two'.

  extinction:
    single_planet_risk: 0.12 # TODO check whether this should be single_ or two_
    decay_rate: 0.55 # Should be a value between 0 and 1. Lower treats the per-planet risk reduction as lower. This
    # value is just a wild guess, though see the note on the equivalent decay_rate for the
    # perils_given_multiplanetary function for an ordering
    min_risk: 0 # For the very long run, if this doesn't tend to become almost 0, longtermism isn't possible
    x_translation: 2 # Trivially, since we define a 'multiplanetary' state as having at least two
    # planets (except for the purpose of regressing to a single planet, which follows the same logic
    # as regressing to 2+ would)

  survival: 0 # Sum of total survival exit probability over all values of q. I treat this as
  # 0 on the grounds that it seems such a precise amount of damage that it's not worth the
  # computation/complexity costs

  preindustrial: 0 # Sum of total preindustrial exit probability over all values of q.
  # While this seems more plausible than going directly to a survival state, it seems unlikely
  # enough to still treat as 0

  industrial: 0 # Sum of total industrial exit probability over all values of q. Again,
  # while this looks somewhat more plausible, it still seems so much less likely than an event which either wipes out
  # humanity or leaves the reaminder with some advanced technology as to be treatable as 0"""

  n_planets:
    # Parameters associated with going from any multiplanetary state to one with fewer planets.
    # You can play with these values at https://www.desmos.com/calculator/4laiomo0jm
    two_planet_risk: 0.2 # Just from eyeballing the Desmos graph
    decay_rate: 0.4 # Also just from eyeballing the Desmos graph
    min_risk: 0.01 # The minimum risk of losing at least one planet before gaining one. Across a
    # Kardashev II civilisation the probability of losing at least one settlement seems like it
    # should remain significant, though given that for the foreseeable future scope for expansion
    # increases cubicly (when you include rocky bodies, and assume after a point even relatively
    # small settlements will have the technology to self-sustain), I would expect it to tend to a
    # low rate relative to the probability of adding a planet
    x_translation: 2 # Trivially, since we define a 'multiplanetary' state as having at least two
    # planets (except for the purpose of regressing to a single planet, which follows the same logic
    # as regressing to 2+ would)
    geometric_base: 1.4 # Used for weighting different numbers of planets we might lose conditional
    # on us losing 1 or more. Higher gives higher probability that given such a loss we'll lose a
    # smaller number of planets

  # perils is just the specific case of transitioning to n=1 planets given our assumptions about
  # transitioning to n planets, so doesn't take any unique params

  interstellar:
    # For this we use our S-curve function. You can interact with the graph with these specific
    # values at https://www.desmos.com/calculator/1wazngfpba
    x_stretch: 6 # Just intuition from eyeballing the function.
    y_stretch: 0.65 # With enough interplanetary colonies, it becomes increasingly likely that we send
    # out interstellar colony ships, since we've proven most of the technology and might run out of
    # space in our solar system.
    x_translation: 1
    sharpness: 2 # My default value when I'm not matching to historical data


